<!DOCTYPE html>
<html lang="en">
  <head>
  	<meta charset="utf-8">
  	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
  	<meta name="description" content="">
  	<meta name="author"      content="map[]">
    
    	<title>Language model for music generation</title>
	<link rel="shortcut icon" href="https://github.com/homerdurand/homer.durand.github.ioimages/gt_favicon.png">

	
	<link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
	
	
	<script defer src="https://use.fontawesome.com/releases/v5.0.11/js/all.js" integrity="sha384-ImVoB8Er8knetgQakxuBS4G3RSkyD8IZVVQCAnmRJrDwqJFYUE4YOv+DbIofcO9C" crossorigin="anonymous"></script>
	
	
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
	
	
  
  <link rel="stylesheet" href="https://github.com/homerdurand/homer.durand.github.io/css/styles.min.34638ffed35228d53c747b891420db1af85104d4c1f5f237e042ca58f80f92001a70f19a703f3a6e92dc0e46f0e0397abae1abe0e1085df1252b83a23c2a68ec.css" integrity="sha512-NGOP/tNSKNU8dHuJFCDbGvhRBNTB9fI34ELKWPgPkgAacPGacD86bpLcDkbw4Dl6uuGr4OEIXfElK4OiPCpo7A==">

   
  

  </head>
  
  <body class="home">

    
      <header id="header">
  <div id="head" class="parallax" data-parallax-speed="2" style="background-image:url('https://github.com/homerdurand/homer.durand.github.ioimages/lake2.jpg');">
    <h1 id="logo" class="text-center">
      <img class='img-circle' src="https://github.com/homerdurand/homer.durand.github.ioimages/moi.jpeg" alt="">
      <span class="title">Homer Durand</span>
      <span class="tagline">Engineer in Applied Mathematics and Computer Science<br>
        <a href="mailto:durand.homer@gmail.com">durand.homer@gmail.com</a>
      </span>
   </h1>
</div>

<nav class="navbar navbar-default navbar-sticky">
    <div class="container-fluid">

        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="true">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>

        <div class="navbar-collapse collapse" id="bs-example-navbar-collapse-1">

            <ul class="nav navbar-nav">
            
                
                <li>
                    <a href="/homerdurand/homer.durand.github.io/">home</a>
                </li>
                
            
                
                <li>
                    <a href="/homerdurand/homer.durand.github.io/home/recentworks/">Academic projects</a>
                </li>
                
            
                
                <li>
                    <a href="/homerdurand/homer.durand.github.io/home/posts/">Personal projects</a>
                </li>
                
            
                
                <li>
                    <a href="/homerdurand/homer.durand.github.io/home/practicals/">Academic Practicals</a>
                </li>
                
            
                
                <li>
                    <a href="/homerdurand/homer.durand.github.io/about/">About</a>
                </li>
                
            
            </ul>

        </div> 
        
    </div>
</nav>

</header>
    
 
    
<main id="main">

	<div class="container">

		<div class="row topspace">
			<div class="col-sm-8 col-sm-offset-2">

 				<article class="post">
					<header class="entry-header">
 						<div class="entry-meta">
               <span class="posted-on">
                  <time class="entry-date published" datetime="January 1, 0001">January 1, 0001</time>
               </span>
 						</div>
 						<h1 class="entry-title"><a href="https://github.com/homerdurand/homer.durand.github.io/home/posts/pproject3/" rel="bookmark">Language model for music generation</a></h1>
					</header>
					<div class="entry-content">
						<p>The objective of this project is to determine a set of
statistical methods for generating musical sequences. Because of the proximity of language and music, the majority of the methods used are borrowed from Natural Language Processing.</p>
<h2 id="contents">Contents</h2>
<ul>
<li>1 Introduction</li>
<li>2 Methodology and difficulties
<ul>
<li>2.1 Objectives</li>
<li>2.2 Data representation</li>
<li>2.3 Music generation models</li>
</ul>
</li>
<li>3 Implementation
<ul>
<li>3.1 Tools and dataset
<ul>
<li>3.1.1 Dataset</li>
<li>3.1.2 Tools</li>
</ul>
</li>
<li>3.2 Data encoding
<ul>
<li>3.2.1 Generic encoding</li>
<li>3.2.2 Atemporal encoding</li>
<li>3.2.3 Temporal encoding</li>
</ul>
</li>
<li>3.3 Stochastic language model
<ul>
<li>3.3.1 Markovian models</li>
<li>3.3.2 Non-vocabulary words</li>
<li>3.3.3 Smoothing and interpolation methods</li>
<li>3.3.4 Markov models for music composition</li>
<li>3.3.5 Limitations of Markov models</li>
</ul>
</li>
<li>3.4 Hidden Markov Model
<ul>
<li>3.4.1 Interest of Hidden State Markov Models</li>
</ul>
</li>
<li>3.5 Performance Measures
<ul>
<li>3.5.1 Shannon Entropy</li>
<li>3.5.2 Cross Entropy</li>
<li>3.5.3 Perplexity</li>
</ul>
</li>
</ul>
</li>
<li>4 Results
<ul>
<li>4.1 Encoding methods</li>
<li>4.2 Markov models
<ul>
<li>4.2.1 First approach</li>
<li>4.2.2 Smoothing methods</li>
<li>4.2.3 Conclusion and future work</li>
</ul>
</li>
</ul>
</li>
<li>5 Conclusion</li>
</ul>
<h2 id="1-introduction">1 Introduction</h2>
<p>The aim is to infer a set of rules governing musical composition in order to generate a
generate a coherent piece.
Having a limited musical culture and in order to make the methods used as generalizable as possible, no musical rules will be used in the creation of the models.
The data will therefore not be annotated.
Being aware that the methods used do not represent the state of the art in terms of music generation, I have chosen to restrict myself to the study of stochastic models and mainly language models (these already allow a very large field of exploration).
This report is an account of my research and readings, a set of personal reflections and the result of several experiments which are certainly not very academic. In spite of my efforts to keep a certain coherence in the development of this report, it has undergone several rather abrupt turns, it is possible that some parts may seem inconsistent with the rest of the report, redundant or that the terms used are not part of the Natural Language Processing jargon.
This report is not the end of the road, there are still many unanswered questions. The implemented models are simple and on the whole not very powerful but have been of great educational value.</p>
<h2 id="2-methodology-and-difficulties">2 Methodology and difficulties</h2>
<h3 id="21-objectives">2.1 Objectives</h3>
<p>This project has been primarily an educational objective, I am aware that none of the models
I am aware that none of the models created within it will be innovative, I am looking to gain new
knowledge and to see how links can be made between two areas that interest me - music theory and
I am primarily interested in gaining new knowledge and seeing how links can be made between two areas that interest me - music theory and automatic language processing.</p>
<p>Several more practical objectives have been set, even if these have undergone several evolutions.
If they have not always been clearly defined we will try here to formulate a version of them
to formulate a version of them that is as precise and comprehensible as possible in order to
to evaluate the results obtained in relation to them.
First of all, we will try to determine the methods of representation of musical data that are best suited to the
data that are best suited to the models used. Several criteria will then be taken into account, the
the two main ones being the simplicity of the representation and the quantity of information
and the amount of information contained in it.
We will then try to create a model that will allow us to describe the data as reliably as possible.
data as reliably as possible. It should therefore be able to infer several rules governing
composition, ranging from the structure of the piece to its rhythmic sequence and harmonization.
from the structure of the piece to its rhythmic sequence and melodic harmonisation. This model should allow us to generate a musical
generate a musical sequence.
Finally, we will try to develop evaluation tools for the generated musical sequences, which will allow us to
We will finally try to develop evaluation tools for the generated musical sequences, which will allow us to evaluate the originality and the accuracy of the creation.</p>
<h3 id="22-data-representation">2.2 Data representation</h3>
<p>Our models require training data in a digital format,
Therefore, data representation is an important step. It raises
several questions:</p>
<ul>
<li>How much information do we want to keep?</li>
<li>In what format?</li>
</ul>
<p>These two questions are in fact interdependent, as the format will depend on the amount of information stored.
the amount of information stored.
It seems likely that the amount of information will have an impact on the performance of the
It seems likely that the amount of information will have an impact on the performance of the model, which will have more difficulty in describing data containing a large amount of information.
information. On the other hand, a large amount of information will allow to
A large amount of information will allow for a more complex and interesting musical sequence.</p>
<h3 id="23-music-generation-models">2.3 Music generation models</h3>
<p>The implemented models should try to meet several criteria:</p>
<ul>
<li>
<p>Generate a musical sequence that is &ldquo;pleasant to listen to&rdquo;.</p>
</li>
<li>
<p>The generated sequence should not &ldquo;plagiarise&rdquo; the training data</p>
</li>
<li>
<p>Have a sense of musical structure and compositional rules</p>
</li>
<li>
<p>Generate coherent sequences of notes or chords</p>
</li>
</ul>
<h2 id="3-implementation">3 Implementation</h2>
<h3 id="31-tools-and-dataset">3.1 Tools and dataset</h3>
<h4 id="311-dataset">3.1.1 Dataset</h4>
<p>The training dataset is a corpus of four-part Bach chorales in
MIDI format for Musical Instrument Digital Interface ( <em>.mid</em> ). The interest of this musical genre
the fact that it is composed of a set of monodic voices, which makes it easy to encode
The interest of this musical genre comes from the fact that it is composed of a set of monodic voices which makes it easier to encode - we will detail why in section [3.2.3].</p>
<h4 id="312-tools">3.1.2 Tools</h4>
<p>The majority of the implementation has been done in Python due to the large amount of
library available in the language.</p>
<ul>
<li>Tools for music analysis</li>
</ul>
<p>As described above, we used as training data
a corpus of MIDI files and for the interaction with these files the <strong>music21</strong> tool which
is a python library dedicated to music analysis. We also used
the <strong>museScore</strong> application for viewing and listening to MIDI files.</p>
<ul>
<li>Markov model</li>
</ul>
<p>We have developed a module dedicated to the use of Markov models. This
module is available on git. We also used the <strong>NLTK</strong> library
library and several of the language models it proposes but the documentation being little
but the documentation is not very abundant, so it was difficult to get used to it. We are working on the implementation of the
smoothing methods described below.</p>
<ul>
<li>Hidden Markov models</li>
</ul>
<p>The hidden Markov models have been implemented with the <strong>HMM</strong> library
library in R because we did not find a satisfactory python library.</p>
<h3 id="32-data-encoding">3.2 Data encoding</h3>
<p>We have implemented two methods of data encoding.</p>
<p>The first generates two independent matrices - one representing the tempo and the other representing the
representing the pitch of the notes played.</p>
<p>The second generates only one matrix representing the pitch of the notes played but here
the pitches depend on the tempo.</p>
<h4 id="321-generic-encoding">3.2.1 Generic encoding</h4>
<p>This encoding generates two matrices (Appendix [3]) each with four lines</p>
<ul>
<li>one line for each voice of the chorale. The first stores the pitch of the notes in
format (Appendix [1]) and the second stores the duration associated with each note.</li>
</ul>
<p>The duration will be a value representing its proportion to a quarter note or its proportion to a beat in a
proportion to a beat in a 4/4 time signature.
will have a duration of 0.5, a sixteenth note of 0.25, etc.)</p>
<h4 id="322-atemporal-encoding">3.2.2 Atemporal encoding</h4>
<p>The atemporal encoding considers as independent the temporal data and the
pitch of the notes.</p>
<ul>
<li><strong>Atemporal Harmonic Encoding</strong> : Notes are stored as a list of four notes [( <em>n</em> 1 <em>,n</em> 2 <em>,n</em> 3 <em>,n</em> 4 ) <em>,</em> ( <em>n</em> 5 <em>,n</em> 6 <em>,n</em> 7 <em>,n</em> 9 ) <em>,&hellip;</em> ]</li>
</ul>
<h4 id="323-encodage-temporel">3.2.3 Encodage temporel</h4>
<p>Cette encodage considère que la durée des notes dépend de leur hauteur.</p>
<ul>
<li><strong>Encodage Harmonique temporel</strong> : Les notes sont stockées comme une liste
d’ensemble de quatre couple (hauteur, durée)
[(( <em>n</em> 1 <em>,d</em> 1 ) <em>,&hellip;,</em> ( <em>n</em> 4 <em>,d</em> 4 )) <em>,</em> (( <em>n</em> 5 <em>,d</em> 5 ) <em>,&hellip;,</em> ( <em>n</em> 8 <em>,d</em> 8 )) <em>,&hellip;</em> ]</li>
<li><strong>Encodage Mélodique temporel</strong> : Les notes sont stockées comme une liste de
couples (hauteur, durée)
[( <em>n</em> 1 <em>,d</em> 1 ) <em>,</em> ( <em>n</em> 2 <em>,d</em> 2 ) <em>,</em> ( <em>n</em> 3 <em>,d</em> 3 ) <em>,&hellip;</em> ]</li>
</ul>
<h3 id="33-modèle-de-langage-stochastique">3.3 Modèle de langage stochastique</h3>
<p>Nous cherchons à appliquer des modèles, généralement utilisés dans le traitement au-
tomatique du langage, à des données musicales. Nous considérons pour cela une
séquence mélodique ou harmonique <em>X</em> = ( <em>X</em> 0 <em>,X</em> 1 <em>,&hellip;,Xn</em> )à valeur dans <em>E</em> , l’espace
des états possibles.</p>
<p>En empruntant le vocabulaire du traitement automatique du langage nous utiliserons
la terminologie suivante :</p>
<ul>
<li>
<p><em>X</em> : Phrase - <em>ici une séquence musicale mélodique ou harmonique</em></p>
</li>
<li>
<p><em>Xi</em> : Mot - <em>ici une note ou un accord</em></p>
</li>
<li>
<p><em>E</em> : Vocabulaire - <em>ici l’ensemble des notes ou accords</em></p>
</li>
<li>
<p>| <em>E</em> |= <em>card</em> ( <em>E</em> ): Taille du vocabulaire - <em>ici nombre de notes ou accords</em></p>
</li>
</ul>
<h4 id="331-modèles-markoviens">3.3.1 Modèles markoviens</h4>
<p>Dans cette section nous étudions l’application de modèles très simples que sont les
modèles markoviens - modèle n-gramme - à la génération de séquence musicale.</p>
<p>Nous faisons l’hypothèse que la probabilité d’apparition d’un mot ne dépend que de
son contexte local, en considérant comme le contexte d’un mot la séquence de mots
antérieure à son apparition. Il existe des modèles considérant également la séquence de
mots ultérieure (modèle bidirectionnel) mais nous ne les étudierons pas ici. On a donc
:</p>
<pre><code>∀ i 0 ,i 1 ,...,in,j ∈ E,
P ( Xn +1= j ) = P ( Xn +1= j | Xn = in,...,X 1 = i 1 ,X 0 = i 0 )
</code></pre><p>Les méthodes qui suivent cherche à inférer cette probabilité d’un corpus musical.</p>
<ul>
<li><strong>Pertinence des modèles markoviens</strong></li>
</ul>
<p>Faisons l’hypothèse que la probabilité d’apparition d’un état dans une séquence - une
note pour les séquences mélodiques ou un ensemble de notes pour les séquences har-
moniques - dépend uniquement des états de la sous-séquence de taille <em>k</em> le précédent.
Cette hypothèse s’appui sur l’idée qu’une oeuvre musicale possède une structure com-
posée de schémas récurrents - que ce soit au niveau de la mélodie ou de l’harmonie. On
peut par exemple observer les degrés d’un morceau de jazz et remarquer que ce dernier
est composé de plusieurs cadences et suite de notes répétitives. On s’attendra par ex-
emple à la suite des accords <em>Dm</em>^7 - <em>G</em>^7 à entendre un accord <em>Cmaj</em>^7 afin de compléter
la célèbre cadence jazz <em>ii</em> − <em>V</em> − <em>I</em>.</p>
<dl>
<dt>Considérons les 16 premières mesures du morceau <em>Autumn Leaves - J.Prévert, J.Kosma</em></dt>
<dt>(Appendix [4]). On remarque dans cette séquence deux niveaux de structures répétitives</dt>
<dd>la première au niveau de la mélodie - les sections <em>A</em> 1 et <em>A</em> 2 étant majoritairement
similaires, la seconde au niveau de l’harmonisation - les sections <em>A</em> 1 et <em>A</em> 2 sont composées
des mêmes cadences (un <em>ii</em> − <em>V</em> − <em>I</em> en <em>Sib</em> majeur suivi d’un <em>ii</em> − <em>V</em> − <em>i</em> en <em>Sol</em> mineur).</dd>
</dl>
<p>On pourrait donc à la vue de cette exemple induire les règles suivantes :</p>
<ul>
<li>A la suite de la séquence de notes (sol, la, si <em>b</em> ), on s’attend à voir apparaître la
note mi <em>b</em>.</li>
<li>A la suite de la séquence d’accord (Cm^7 , F^7 ), on s’attend à voir apparaître l’accord
B <em>bMaj</em>^7.</li>
</ul>
<p>C’est ce type de règles que l’on cherche à induire au travers des modèles markoviens.</p>
<ul>
<li>Markov models**.</li>
</ul>
<p>Consider the sentence <em>X</em> = <em>X</em> 0 <em>,X</em> 1 <em>,&hellip;,Xn</em> with value in <em>E</em> , the space of possible states.
We consider here <em>X</em> as being a Markov chain (or Markov process),
i.e. the state of <em>Xn</em> +1 depends only on the value of the state of <em>Xn</em> :</p>
<pre><code>∀ i 0 ,i 1 ,...,in,j ∈ E,
P ( Xn +1= j ) = P ( Xn +1= j | Xn = in,...,X 1 = i 1 ,X 0 = i 0 )
= P ( Xn +1= j | Xn = in )
</code></pre><p>Let us call <em>pi,j</em> the transition probability from state <em>i</em> to state <em>j</em> :</p>
<pre><code>pi,j = P ( Xn +1= j | Xn = i )
= P ( X 1 = j | X 0 = i ) by property of Markov processes
</code></pre><p>We call <em>P</em> = ( <em>pi,j</em> ) <em>i,j</em> ∈ <em>E</em> the transition matrix of X.</p>
<ul>
<li>Markovian models of order K</li>
</ul>
<p>We will now generalize the Markov chain principle. The models described here are also
here are also known as n-gram models^1 - the Markov models described above are therefore big
models described above are therefore bigram models. We always consider a
sentence <em>X</em> with a value in <em>E</em> but now the state of <em>Xn</em> +1 depends on the values of the states
of <em>Xn,&hellip;,Xn</em> - <em>k</em> with <em>k</em> ≥ 0 :</p>
<pre><code>∀ i 0 ,i 1 ,...,in,j ∈ E,
P ( Xn +1= j ) = P ( Xn +1= j | Xn = in,...,X 1 = i 1 ,X 0 = i 0 )
= P ( Xn +1= j | Xn = in,Xn - 1 = in - 1 ,...,Xn - k = in - k )
</code></pre><p>Let <em>wk</em> = ( <em>i</em> 0 <em>,i</em> 1 <em>,&hellip;,ik</em> ), let us call <em>pwk,j</em> the transition probability of the state sequence
<em>wk</em> = <em>i</em> 0 <em>,i</em> 1 <em>,&hellip;,ik</em> to the state <em>j</em> :</p>
<p><em>pwk,j</em> = <em>P</em> ( <em>Xn</em> +1= <em>j</em> | <em>Xn</em> = <em>i</em> 0 <em>,Xn</em> - 1 = <em>i</em> 1 <em>,&hellip;,Xn</em> - <em>k</em> = <em>ik</em> )</p>
<pre><code>= P ( Xk +1= j | Xk = ik,...,X 1 = i 1 ,X 0 = i 0 ) by property of Markov processes
</code></pre><p>We then call <em>Pk</em> = ( <em>pwk,j</em> ) <em>wk,j</em> ∈ <em>EkxE</em> the k-order transition matrix of <em>X</em>.</p>
<p>(^1) An n-gram is a sub-sequence of n elements</p>
<h4 id="332-non-vocabulary-words">3.3.2 Non-vocabulary words</h4>
<p>More concretely, when we want to test our model on data that it has never seen (data from a previous
(test data) it is possible that some words are not part of the training vocabulary.
of the training vocabulary. These words are then said to be out-of-vocabulary words ( <em>Out-
Of-Vocabulary words - OOV</em> ) or outside the corpus ( <em>Out-Of-Corpus words - OOC</em> ). These words
words pose a problem in measuring the performance of the model because the probability of an
word <em>OOC</em> is zero and therefore the probability of the test sequence (on which the majority of the
the majority of methods for evaluating the performance of a model) is zero. There is a
a simple method to solve this problem: We create a vocabulary from a part of the training
of the training sequence (e.g. 80%) and then add a word &lsquo;<!-- raw HTML omitted -->&rsquo; to it.
<!-- raw HTML omitted -->'. The rest of the training data is used to estimate the probability
of the set of <em>OOC</em> &lsquo;<!-- raw HTML omitted -->&rsquo; words in the same way as for all other
words.</p>
<h4 id="333-smoothing-and-interpolation-methods">3.3.3 Smoothing and interpolation methods</h4>
<p>As the order of the model used increases, the number of n-grams that can be
be observed increases exponentially. Consider a sequence <em>X</em> ̄ taking its values in _E
values in <em>E</em> , for a model of order k we obtain| <em>E</em> | <em>k</em> n-grams observable for this sequence (with| <em>E</em> | <em>k</em>).
this sequence (with| <em>E</em> | = <em>card</em> ( <em>E</em> )). It is possible that words (part of the
It is possible that words (part of the training vocabulary) appear in an unknown context. Their probability
The probability of the words appearing in an unknown context is then again zero and it becomes impossible to measure the performance of the model.
performance of the model.</p>
<p>To overcome this problem, there are several so-called smoothing methods.</p>
<ul>
<li>Laplace smoothing</li>
</ul>
<p>One of the simplest is additive smoothing or Laplace smoothing which simply consists of
to initialise each occurrence <em>O</em> (( <em>wk,j</em> ))to a fixed value <em>α</em>. Thus the probability that
<em>j</em> appears after <em>wk</em> is now :</p>
<pre><code>pwk,j =
</code></pre><pre><code>O ( wk,j ) + α
O ( wk ) + αd
</code></pre><p>Where <em>d</em> is the size of the sequence.</p>
<ul>
<li>Smoothing by backward order (** **_Backoff_** **)**</li>
</ul>
<p>Another method is to backtrack the order of the model until it returns a non-zero probability.
a non-zero probability. Let&rsquo;s take the example of a third-order model for which we want to
predict the element <em>wi</em> following the elements <em>wii</em> &ndash;^1 <em>k</em> = <em>wi</em> - <em>k,wi</em> - <em>k</em> +1 <em>,&hellip;,wi</em> - 1 and we have <em>pwi</em> - 1
<em>i</em> - <em>k</em> - 1 <em>,wi</em></p>
<h5 id="heading">=</h5>
<p>0, then we look at <em>pwii</em> &ndash;^1 <em>k,wi</em> and so on until we have a non-zero probability of occurrence</p>
<p>non-zero. We are sure to get a non-zero probability because the word <em>wi</em> is part of the training vocabulary
training vocabulary, its probability of occurrence out of context is non-zero - it is
It is <em>O</em> ( <em>Nwi</em> )with <em>N</em> the size of the training sequence.</p>
<ul>
<li>Interpolated Smoothing Methods</li>
</ul>
<p>Finally, the last method consists in taking a linear combination of the kth order Markovian
models of order k and lower. We consider the following mixed model of order at most k
at most:</p>
<pre><code>P ( Xn +1= j ) = P ( Xn +1| Xnn - k )
= λ 0 P ( Xn +1| Xn ) + λ 1 P ( Xn +1| Xnn - 1 ) + ... + λkP ( Xn +1| Xnn - k )
</code></pre><p>With</p>
<pre><code>∑ k
i =0 λi = 1and X
</code></pre><pre><code>n
n - k = Xn,Xn -^1 ,...,Xn - k
</code></pre><p>Where the linear coefficients <em>λi</em> can be estimated according to several criteria. Our
objective is sequence prediction, we will estimate them by trying to maximise our
our performance measure.</p>
<p>We can also consider that the coefficients <em>λi</em> depend on their context, we have
then :</p>
<pre><code>P ( Xn +1= j ) = P ( Xn +1| Xn - k,Xn - k +1 ,...,Xn )
= λ 0 ( Xnn - k ) P ( Xn +1| Xn ) + λ 1 ( Xnn - k ) P ( Xn +1| Xnn - 1 ) + ...
+ λk ( Xnn - k ) P ( Xn +1| Xnn - k )
</code></pre><p>More practically, we will use a part of the training data (say
80% for example) for the evaluation of probabilities and another for optimising the
coefficients <em>λ</em> so that they maximise the performance measure of our model. In
the expectation-maximisation algorithm will be used to find a local optimum of _λi
of <em>λi</em>.</p>
<p>It has been shown (KNESER and NEY 1995) that the majority of smoothing methods are
of the form :</p>
<pre><code>α ( wii --^1 k +1) If O ( wii - k +1) &gt; 0
psmooth ( wi | wii -- k^1 +1) =
γ ( wii -- k^1 +1) psmooth ( wi | wii -- k^1 +2) If O ( wii - k +1) = 0
</code></pre><ul>
<li>Interpolated Kneser-Ney Smoothing</li>
</ul>
<p>Interpolated Kneser-Ney Smoothing (IKNS) is probably the most widely
the most referenced smoothing method in the literature and seems to be one of the most efficient (CHEN et al.
(CHEN et AL. 1998 [Che]).</p>
<p>The model has the following form:</p>
<pre><code>max { O ( wii - k +1)- D, 0 }
O ( wii -- k^1 +1)
</code></pre><pre><code>If O ( wii - k +1) &gt; 0
</code></pre><pre><code>pKN ( wi | wii -- k^1 +1) =
γ ( wii -- k^1 +1) pKN ( wi | wii --^1 k +2) If O ( wii - k +1) = 0
</code></pre><pre><code>With
</code></pre><pre><code>∑
γ ( wii --^1 k +1) = 1and D &lt; 0
</code></pre><h4 id="334-markov-models-for-music-composition">3.3.4 Markov models for music composition</h4>
<ul>
<li>Learning</li>
</ul>
<p>Consider a sequence of (melodic) notes or a set of (harmonic) notes
<em>X</em> whose set of states <em>E</em> is the set of possible notes for a melodic sequence
melodic sequence and the set of possible note sets for a harmonic sequence.
From this sequence we induce a transition matrix of order k <em>Pk</em>. We call <em>Pk</em> our
model of order k.</p>
<p>The elements of the matrix <em>pwk,j</em> are estimated by examining a sample of the data (training sample).
data (training sample). Let <em>O</em> ( <em>w</em> )and <em>O</em> ( <em>w,j</em> ) be, respectively, the number of occurrences
of occurrences of the sequence <em>w</em> and of <em>w</em> followed by <em>j</em> in the training sample. We
then estimates <em>pwk,j</em> by its maximum likelihood estimation
:</p>
<pre><code>pwk,j =
</code></pre><pre><code>O ( wk,j )
O ( wk )
</code></pre><p>This step is called learning.</p>
<ul>
<li>Prediction**</li>
</ul>
<p>We now seek to predict a state following a sub-sequence <em>X</em> ∗= ( <em>X</em> 0 =
<em>i</em> 0 <em>,X</em> 1 = <em>i</em> 1 <em>,&hellip;,Xk</em> = <em>ik</em> ) = <em>wk</em>.</p>
<p>The simplest method is to choose the <em>argMaxj</em> ( <em>pwk,j</em> ). This method allows
This method maximizes the prediction accuracy (rate of correctly predicted states) over the
validation sequence, on the other hand it will strongly reduce the cardinal of the set of
states (vocabulary size) by always choosing over-represented states in the training sequence.
in the training sequence.</p>
<p>Another method is to randomly draw the probable states weighted by their probability of occurrence.
by their probability of occurrence. The accuracy of the prediction is therefore greatly reduced, but a
The accuracy of the prediction is therefore greatly reduced, but an output vocabulary of similar size to that of the input vocabulary is retained.
input vocabulary.</p>
<p>Finally, we can try to reconcile the conservation of the size of the vocabulary and the accuracy of the prediction by making a random draw.
Finally, we can try to reconcile the conservation of the size of the vocabulary and the accuracy of the prediction by making a weighted random draw among the <em>p</em> most probable states.</p>
<p>In the following, we will note <em>Pk</em> ( <em>wk</em> ) the prediction of the model of order k for the sequence
<em>X</em> ∗= <em>i</em> 0 <em>,i</em> 1 <em>,&hellip;,ik</em> = <em>wk</em> :</p>
<pre><code>Pk ( X ∗) = j
j = gen ( p ( wk ) ,j )
Where gen ( p ( wk ) ,j )is a prediction of type argmaxj ( p ( wk ) ,j )
</code></pre><ul>
<li>Generation**</li>
</ul>
<p>So far, we have developed a model - induced by a learning sequence - that allows us to predict a state from a
that allows us to predict a state from the previous <em>k</em> states. From this we can
generate a state sequence of size <em>l</em> from our model and an input sequence of size n greater than or equal to
input sequence of size n greater than or equal to k.
Let <em>X</em> ̄= <em>i</em> 0 <em>,i</em> 1 <em>,&hellip;,in</em> be a sequence of size n and <em>Pk</em> our model:</p>
<pre><code>For 0 ≤ i ≤ l - n :
X ∗= Xn - k,Xn - k +1 ,...,Xn
Xn +1= P ( X ∗)
X ̄= ( X ∗ ,Xn +1)
</code></pre><p>Thus, we generated a sequence of size <em>l</em> from our training model.</p>
<h4 id="335-limitations-of-markov-models">3.3.5 Limitations of Markov models</h4>
<p>The main limitation of the Markov models described above is that they identify only local
only local structures of sequences. They do not, by definition
They do not, by definition, allow the identification of long-distance dependencies and the global structure of a piece.</p>
<p>Hidden Markov models are introduced to compensate for this lack.</p>
<h3 id="34-hidden-markov-model">3.4 Hidden Markov Model</h3>
<p>The idea of Hidden Markov Models is to create a level of abstraction in the
classical Markov models. The idea being that the observed sequence <em>X</em> = <em>x</em> 1 <em>,x</em> 2 <em>,&hellip;,xN</em>
is emitted by a set of unobservable hidden states. A simple example of a
A simple example of a Markov model with hidden states is that of the cheater: A cheater uses two coins to make
one balanced and the other piped and discreetly changes the coin without the observer noticing.
without the observer noticing. We therefore observe a sequence [P, P, F,
P, F, F, F, P, F, P, P, P, P, P, &hellip;] and we try to find out which piece was used at which moment, i.e. we try to find out which piece was used at which moment.
at which time, i.e. we are looking for the sequence [not Piped, not Piped, not Piped, not Piped,
no Piped, no Piped, no Piped, no Piped, Piped, Piped, Piped, &hellip;] where each event corresponds to an observation.
events corresponds to an observation.</p>
<p>A hidden state Markov model is a set <em>S,</em> Π <em>,A,B</em> with :</p>
<ul>
<li>S the set of hidden states</li>
<li><em>πi</em> the probability that the initial state is <em>si</em></li>
<li>A the transition matrix of hidden states</li>
<li>B the emission matrix, <em>bi,k</em> is the probability of emitting symbol k in state i</li>
</ul>
<p>Under constraint</p>
<pre><code>∑
iπi = 1,∀ i
</code></pre><pre><code>∑
jai,j = 1and∀ i
</code></pre><pre><code>∑
kbi,k = 1
</code></pre><p>There are three typical problems that one seeks to solve with Markov models
models:</p>
<ul>
<li>Compute the probability of a sequence given a model</li>
<li>Find the most likely sequence of hidden states that generated the sequence of
of observations</li>
<li>Infer the set <em>S,</em> Π <em>,A,B</em> of a sequence of observations</li>
</ul>
<h4 id="341-interest-of-hidden-state-markov-models">3.4.1 Interest of hidden state Markov models</h4>
<p>The use of hidden Markov models for music sequence harmonisation
has been studied on several occasions [AW]. This is due to the level of abstraction
that it allows to remove from a melodic sequence (observed sequence). Indeed, one can
a melodic sequence can be considered to be emitted by hidden states (harmonic structure of the sequence).
harmonic structure of the sequence). The Viterbi algorithm will then allow us to infer the
harmonic structure of the melody.</p>
<p>In our case we envisage that the hidden Markov models will allow us to
In our case we envisage that the hidden Markov models will allow us to identify more global structures such as themes or repeated patterns.</p>
<h3 id="35-performance-measures">3.5 Performance measures</h3>
<p>In this section, we seek to determine a performance measure adapted to our problem
to our problem, i.e. whether our model is capable of generating a good musical
musical sequence. In general, we pose a generation problem as a
a prediction problem. Indeed, if our model is able to &ldquo;well predict&rdquo; a
test sequence that it has never seen, then it will be able to generate a good
musical sequence.</p>
<h4 id="351-shannon-entropy">3.5.1 Shannon Entropy</h4>
<p>In information theory^2 the Shannon entropy is used to measure the amount of information contained in a source of information.
of information contained in an information source. Intuitively it can be seen as
as the minimum number of bits necessary to encode this source of information.</p>
<p>(^2) Element of Information Theory [CT].</p>
<p>Given a discrete random variable <em>X</em> with value in <em>E</em> , each element <em>x</em> of which has a probability of occurrence _p
has a probability of occurrence <em>p</em> ( <em>x</em> ), it is calculated as follows:</p>
<pre><code>H ( X ) =-
</code></pre><pre><code>∑
</code></pre><pre><code>x ∈ E
</code></pre><pre><code>p ( x ) logb ( p ( x ))
</code></pre><p>We will take <em>b</em> = 2 so that the value obtained is in bits.</p>
<p>If <em>X</em> = <em>x</em> 1 <em>,x</em> 2 <em>,&hellip;,xn</em> is a discrete stochastic process, Shannon&rsquo;s theorem
McMillan Breiman theorem tells us that:^3</p>
<h5 id="-">-</h5>
<h5 id="1">1</h5>
<pre><code>n
</code></pre><pre><code>log 2 P ( x 1 ,x 2 ,..,xn )- &gt; H ( X )when n- &gt; ∞
</code></pre><h4 id="352-cross-entropy">3.5.2 Cross Entropy</h4>
<p>From this idea of a minimal number of bits to store a source of information, emerges
the idea of cross-entropy which allows to compare two probability distributions <em>p</em> and <em>q</em>
with values in the same set <em>E</em>. It is useful when we do not know the distribution p that generated our sequence.
distribution p that generated our sequence. It allows us to use an approximation
m of p.</p>
<pre><code>H ( p,m ) =-
</code></pre><pre><code>∑
</code></pre><pre><code>x ∈ E
</code></pre><pre><code>p ( x ) logb ( m ( x ))
</code></pre><p>Again, if <em>X</em> = <em>x</em> 1 <em>,x</em> 2 <em>,&hellip;,xn</em> is a stationary discrete stochastic process
and ergodic, the Shannon-McMillan-Breiman theorem tells us that :</p>
<h5 id="--1">-</h5>
<h5 id="1-1">1</h5>
<pre><code>n
</code></pre><pre><code>log 2 m ( x 1 ,x 2 ,..,xn )- &gt; H ( X )when n- &gt; ∞
</code></pre><p>The interest of cross-entropy is that it gives us a lower bound on the entropy
of the actual distribution of our data. We can get a good estimate of the
cross-entropy of our model <em>m</em> = <em>P</em> ( <em>Xi</em> +1| <em>Xi,&hellip;,X</em> 2 <em>,X</em> 1 )on a sufficiently large test sequence: <em>P</em> ( <em>Xi</em> +1| <em>Xi,&hellip;,X</em> 2 <em>,X</em> 1 )
sufficiently large:</p>
<h5 id="h--x---">H ( X ) =-</h5>
<h5 id="1-2">1</h5>
<h5 id="n">N</h5>
<pre><code>log 2 P ( x 1 ,x 2 ,..,xN )
</code></pre><h4 id="353-perplexity">3.5.3 Perplexity</h4>
<p>More concretely, the measure generally used for the evaluation of language models is perplexity.
language models is perplexity. It is based on the idea that a good model is one that
maximising the probability of occurrence of the test sequence <em>X</em> = <em>x</em> 1 <em>,x</em> 2 <em>,..,xn</em> which is
given by <em>P</em> ( <em>X</em> ) =</p>
<p>∏ <em>N
i</em> =1 <em>P</em> ( <em>xi</em> ). The complexity of a sequence <em>X</em> is calculated as follows
:</p>
<p>(^3) Under the assumption of ergodicity</p>
<h5 id="pp--x---p--x--">PP ( X ) = P ( X )-</h5>
<pre><code>1
n
</code></pre><p>This measure can easily be related to the cross-entropy described above:</p>
<pre><code>PP ( X ) = P ( x 1 ,x 2 ,...,xN )-
N^1
</code></pre><pre><code>= ( P ( x 1 ) P ( x 2 ) ...P ( xN ))-
</code></pre><pre><code>1
N
</code></pre><pre><code>= 2( log^2 ( P ( x^1 ) P ( x^2 )) ...P ( xN ))
</code></pre><pre><code>- N^1
</code></pre><h5 id="-2-">= 2-</h5>
<pre><code>N^1 ∑ n
i =0 logb ( P ( x ))
= 2 H ( X )
</code></pre><p>Perplexity is interesting because it gives us a concrete performance measure.
We can, in fact, see perplexity as the average number of words that can occur
following any word. If our model is &ldquo;bad&rdquo; (all its states are
If our model is &ldquo;bad&rdquo; (all its states are equiprobable) then its perplexity will be close to the size of its vocabulary.</p>
<h2 id="4-results">4 Results</h2>
<h3 id="41-encoding-methods">4.1 Encoding methods</h3>
<p>We will first evaluate the different methods of encoding the
data. Let us recall that our corpus is composed of a set of Bach chorales
chorales in MIDI format. The four encoding methods studied are</p>
<ul>
<li>Temporal harmonic encoding</li>
<li>Atemporal harmonic encoding</li>
<li>Temporal melodic encoding</li>
<li>Atemporal melodic encoding</li>
</ul>
<p>Our evaluation will focus on two criteria: the size of the vocabulary and the linguistic coverage
coverage it generates. As the different encoding methods have led to variations in
the size of the sequences, the vocabulary will be evaluated in relation to the size of the
size of the sequence that generated it.</p>
<ul>
<li>Vocabulary size</li>
</ul>
<p>As can be seen in figure [5], harmonic encoding results in a very large vocabulary size.
vocabulary size. This is consistent with the number of possible arrangements
This is consistent with the number of possible arrangements between notes for this type of encoding: Let <em>N</em> be the set of possible notes,
Let <em>T</em> be the set of duration values of these notes. A melodic line without durations takes
its values in <em>M</em> = <em>N</em> and a melodic line with durations takes its values in
<em>Mt</em> = <em>NxT</em> of cardinal| <em>Mt</em> |=| <em>N</em> | <em>x</em> | <em>T</em> |. A harmonic sequence without durations composed of
of 4 melodic lines takes its values in <em>H</em> = <em>N</em>^4 of cardinal | <em>H</em> | = | <em>N</em> |^4 and
a harmonic sequence with durations takes its values in <em>Ht</em> = <em>M</em>^4 of cardinal
| <em>H</em> |=| <em>Mt</em> |^4.
Our corpus contains 60 different notes that can have 51 different durations:</p>
<h5 id="---m--n--60">- | M |=| N |= 60</h5>
<ul>
<li>|Mt_ |=| _N_ |×| _T_ |= 3060</li>
<li>| <em>H</em> |=| <em>N</em> |^4 = 12960000</li>
<li>| <em>Ht</em> |= (| <em>N</em> |×| <em>T</em> |)^4 = 8_._ 8 × 1013</li>
</ul>
<p>Compared to the length of their sequences, the vocabulary of har-
In comparison to the length of their sequences, the vocabulary of har- monic sequences takes its values from a very large cardinal set and it is therefore not surprising that the size of the vocabulary is very small.
that the size of the vocabulary is of the same order of magnitude as the sequence that
that generated it.</p>
<ul>
<li>Linguistic coverage</li>
</ul>
<p>Another important aspect in the choice of data encoding is the linguistic coverage that the vocabulary
coverage that the vocabulary generates. The language coverage makes it possible to observe
whether the words in the sequence have uniform frequencies of occurrence or not - in concrete terms, this is a simple plot of the frequency of occurrence of the words.
It is a simple plot of the sum of the frequency of the vocabulary sorted from the highest to the
frequencies to the lowest. A linear language coverage corresponds to
a vocabulary with a uniform distribution.
A linear language coverage seems to be of little interest in the context of stochastic
linguistic model because these models try to identify rules in the distribution of the
vocabulary^4</p>
<p>We see here that the encodings with the most interesting linguistic coverage are
harmonic encodings without durations and melodic encodings with durations.</p>
<p>Because of its linguistic coverage and the reasonable size of its vocabulary we will carry out our
We will carry out our experiments on the melodic encoding with durations. It could be interesting to
interesting to develop experiments with the other types of encoding in order to see if our
see if our assumptions about their poor compatibility with Markovian models are confirmed.
models are confirmed.</p>
<h3 id="42-markovian-models">4.2 Markovian models</h3>
<p>Once the training is done, it is important to define one or more performance measures that will allow us to
that will allow us to evaluate the performance of the model. As
As we have already explained, our goal of music generation can be reduced to a prediction
prediction problem: if our model is able to &ldquo;well predict&rdquo; unknown musical sequences then
If our model is able to &ldquo;predict well&rdquo; unknown musical sequences, then it should generate good sequences.</p>
<h4 id="421-first-approach">4.2.1 First approach</h4>
<p>One of the most commonly used performance measures for assessing the performance of statistical models on qualitative data is accuracy, i.e. the
statistical models on qualitative data is accuracy, i.e. the rate of correctly predicted
the rate of correctly predicted variables. We have therefore evaluated the performance of
of the Markovian models on our test sample (20% of the data) with regard to their
their accuracy. As the set of pairs (scores, durations) is large, it may be rare that the model
The set of pairs (scores, durations) is large, so it may be rare that the model correctly predicts the next pair, but it may still have a high probability of occurrence.
but it may still have a high probability of occurrence. For this purpose, we also calculate the top-k
accuracy, i.e. we look at whether the next pair is in the k
most likely elements.</p>
<p>Several things emerge from the analysis of this measure (Appendix [7]): Firstly
First, we see that the accuracy is low and that only a second-order model can raise it
above 25%. Then we can see that the value of the accuracy of the models seems to
to collapse as their order increases. This is a very important point in the creation of
creation of complete language models. Indeed, as we have already explained, it is
It is highly likely that some elements of the test sequence will appear in a context they never
context that they had never seen during the training phase. This probability</p>
<p>(^4) We did not find any literature confirming or refuting this point, further research and/or experiments might be interesting.
research and/or experimentation might be of interest.</p>
<p>This probability (^4) increases as the number of <em>Out-of-Vaocabulary</em> words increases because a context
containing an <em>OOC</em> word is necessarily an unknown context. Each n-gram
containing at least one <em>OOC</em> element will then be unknown to the model and the model will be
and the model will be unable to predict anything from them. The problem thus increases exponentially with the size of the n
The problem increases exponentially with the size of the n-grams - hence with the order of the model.</p>
<h4 id="422-smoothing-methods">4.2.2 Smoothing methods</h4>
<p>We have previously described three methods for solving the problem of unknown con-
We have previously described three methods for solving the problem of unknown texts, additive smoothing methods, backward methods and interpolated models.
interpolated models. Here we analyse the results of additive smoothing - which seems to be the simplest and most intuitive method - to solve the problem of unknown con- texts.
and the Kneser-Ney method, which is considered to be one of the most efficient methods.
method, which is considered to be the most efficient.</p>
<p>It would seem from our results (Appendix [11]) that the Laplace and Kneser-Ney smoothing methods are the most efficient.
Laplace and Kneser-Ney smoothing methods provide the same order of magnitude of perplexity on the test sequence (about
perplexity on the test sequence (about 50) which is an interesting perplexity
in view of the size of our vocabulary. Indeed, while we would need on average
between 9 and 10 bits to encode our test sequence if all the states were equiproba- tive (the entropy of the test sequence is not the same).
(the entropy of this sequence would then be <em>log</em> 2 (700) = 9_._ 45 ) only 5 to 6 bits are on average necessary to encode the sequence.
to encode the test sequence given the distribution generated by our model.
generated by our model.</p>
<h4 id="423-conclusion-and-future-work">4.2.3 Conclusion and future work</h4>
<p>The analysis of the generated sequences does not enter into the analysis of our results
because, as can be seen from this example (Appendix [9]), it is easy to create a model
over-learn our data and generate a melodic sequence that may seem interesting but is only a
interesting but which will only be a copy of a part of the training data (A
(A very high order Laplace model will for example do the job very well).</p>
<p>Finally, with the methods studied, we were able to use language models
in the context of music sequence generation and we think that these are quite relevant for this kind of
are quite relevant for this kind of task. Even if the results obtained do not seem to be
we have succeeded in creating models capable of inferring certain local structures from
inferring certain local structures from a training corpus. In the Appendix you will find
In the Appendix you will find the partitions of some of the sequences created by our models.</p>
<p>A large part of the methods and ideas that have been made explicit in this report have not yet been
not yet developed or not sufficiently developed for testing to be carried out under good conditions.
to be carried out under good conditions. We are currently working on the implementation of
of hidden Markov models in R and the creation of a more scalable python module of the
smoothing methods than the one used for this project (module <em>lm</em> of
NLTK) and allowing more experimentation.</p>
<h2 id="5-conclusion">5 Conclusion</h2>
<p>Overall this project was very formative. It allowed me to develop
several theoretical knowledge concerning language models, information theory, encoding
information theory, data encoding and even deep-learning - I spent several weeks studying
recurrent neural networks and LSTMs, which seem to be very efficient in music
generation, before refocusing on the language models I had originally studied.
language models that I had initially studied.</p>
<p>But this project has above all allowed me to realise the difficulty of answering
to an unclear problem and lacking a framework. A large number of lines of
lines of code were thrown away because I had not taken enough time to implement my models.
my models. Even if I don&rsquo;t have the necessary rigour to implement complex models
models, I have been able to realise over the last few months that it is necessary to
work in good conditions and provide solid, relevant and above all reproducible results.
reproducible results.</p>
<h2 id="references">References</h2>
<p>[AGR] Leif Azzopardi, Mark Girolami, and Keith Van Rijsbergen. Investigating
the Relationship between Language Model Perplexity and IR Precision-Recall
Measures. page 3.</p>
<p>[AW] Moray Allan and Christopher Williams. Harmonising Chorales by Probabilistic
Inference. page 8.</p>
<p>[BPX+] Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean.
Large Language Models in Machine Translation. page 10.</p>
<p>[Che] Stanley F Chen. An Empirical Study of Smoothing Techniques for Language
Modeling. page 64.</p>
<p>[Con] Darrell Conklin. Music Generation from Statistical Models. page 6.</p>
<p>[CT] Thomas M Cover and Joy A Thomas. <em>ELEMENTS OF INFORMATION
THEORY</em>.</p>
<p>[Dan19] Roger B Dannenberg. Week 5 – Music Generation and Algorithmic Composi-
tion. page 38, 2019.</p>
<p>[Jam] Frankie James. Modified Kneser-Ney Smoothing of n-gram Models. page 19.</p>
<p>[MQ09] Robert C. Moore and Chris Quirk. Improved smoothing for N-gram language
models based on ordinary counts. In <em>Proceedings of the ACL-IJCNLP 2009
Conference Short Papers on - ACL-IJCNLP ’09</em> , page 349, Suntec, Singapore,</p>
<ol start="2009">
<li>Association for Computational Linguistics.</li>
</ol>
<p>[noa] <em>Experimental music; composition with an electronic computer : Hiller, Lejaren,
1924-1994 : Free Download, Borrow, and Streaming : Internet Archive</em>.</p>
<p>[PWM] Dan Ponsford, Geraint Wiggins, and Chris Mellish. Statistical learning of
harmonic movement. page 35.</p>
<p>[YB17] Adrien Ycart and Emmanouil Benetos. A STUDY ON LSTM NETWORKS
FOR POLYPHONIC MUSIC SEQUENCE MODELLING. page 7, 2017.</p>
					</div>
				</article>

			</div>
		</div> 

		<div class="row">
			<div class="col-sm-8 col-sm-offset-2">

				<div id="share">
					
				</div>
			</div>
		</div> 
		<div class="clearfix"></div>

		<div class="row">
			<div class="col-sm-8 col-sm-offset-2">

				<div id="comments">
					
				</div>
			</div>
		</div> 
		<div class="clearfix"></div>

	</div>	

</main>

    
    
      <footer id="footer">
	<div class="container">
		<div class="row">
			
			<div class="col-md-3 widget">
				<h3 class="widget-title">Contact</h3>
				<div class="widget-body">
					<p>&#43;33 6 99 97 13 67<br>
					<a href="mailto:durand.homer@gmail.com">durand.homer@gmail.com</a><br>
					<br>
					15 rue Delphine Seyrig 75019 Paris
					</p>
				</div>
			</div>
			

			
			<div class="col-md-3 widget">
				<h3 class="widget-title">Follow me</h3>
				<div class="widget-body">
					<p class="follow-me-icons">
            
							
            
							
            
							
            
							
            
							
								<a href="https://www.linkedin.com/in/homer-durand-m2stats" target="_blank"><i class="fab fa-linkedin fa-1x"></i></a>
							
            
							
            
							
								<a href="https://github.com/homerdurand" target="_blank"><i class="fab fa-github fa-1x"></i></a>
							
            
							
								<a href="mailto:durand.homer@gmail.com" target="_blank"><i class="fas fa-envelope-square fa-1x"></i></a>
							
            
					</p>
				</div>
			</div>
			

			

			

		</div> 
	</div>
</footer>

<footer id="underfooter">
	<div class="container">
		<div class="row">

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p>15 rue Delphine Seyrig 75019 Paris</p>
				</div>
			</div>

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p class="text-right">
						Copyright &copy; 2021, Homer Durand<br>
						Design: <a href="http://www.gettemplate.com" rel="designer">Initio by GetTemplate</a> - 
						Powered by: <a href="https://gohugo.io/" rel="poweredby">Hugo</a>
					</p>
				</div>
			</div>

		</div> 
	</div>
</footer>




<script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>




<script src="https://github.com/homerdurand/homer.durand.github.io/js/bundle.min.f4a965ad0a8118e32f8f0f158ff3aadbacf700934c22286a1a5b245105e9006da73a873b001a160db22498909c1df14d1f835dba5ad76f80b32b0234182b2a58.js" integrity="sha512-9KllrQqBGOMvjw8Vj/Oq26z3AJNMIihqGlskUQXpAG2nOoc7ABoWDbIkmJCcHfFNH4NdulrXb4CzKwI0GCsqWA=="></script>

<script id="dsq-count-scr" src="//homer-durand.com/count.js" async></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'GA-000000000-0', 'auto');
  ga('send', 'pageview');
</script>

</body>
</html>

    
  </body>
  
</html>