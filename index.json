[{"content":"\u003cp\u003eðŸ“„ \u003ca href=\"https://arxiv.org/abs/2403.01865\"\u003ePaper accepted at AISTATS 2025\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe term \u003cem\u003ediluted causality\u003c/em\u003e comes from \u003ca href=\"https://arxiv.org/pdf/1812.08233\"\u003eBÃ¼hlmann (2018)\u003c/a\u003e, inspired by a suggestion from \u003ca href=\"https://statistics.wharton.upenn.edu/profile/edgeorge/\"\u003eEdward George\u003c/a\u003e. It reflects the notion that while causal mechanisms are expected to be invariant under all possible interventions, a weaker form of invarianceâ€”\u003cem\u003ediluted causality\u003c/em\u003eâ€”can be more desirable in practice. This weaker notion restricts the set of interventions to those most relevant for predictive generalisation.\u003c/p\u003e\n\u003cp\u003eThis project investigates the interplay between \u003cstrong\u003ecausality\u003c/strong\u003e, \u003cstrong\u003einvariance\u003c/strong\u003e, and \u003cstrong\u003eout-of-distribution (OOD) generalisation\u003c/strong\u003e. It builds upon influential work by Peter BÃ¼hlmann, Nicolai Meinshausen, Jonas Peters, Dominik RothenhÃ¤usler, Rune Christiansen, and others.\u003c/p\u003e\n\u003cp\u003eWe especially recommend reviewing these foundational papers to understand the core concepts connecting causality and robustness:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://academic.oup.com/jrsssb/article/78/5/947/7040653\"\u003ePeters et al. (2016)\u003c/a\u003e â€“ \u003cem\u003eCausal Inference by Using Invariant Prediction: Identification and Confidence Intervals\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/1812.08233\"\u003eBÃ¼hlmann (2018)\u003c/a\u003e â€“ \u003cem\u003eInvariance, Causality and Robustness\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ieeexplore.ieee.org/document/8439889\"\u003eMeinshausen (2018)\u003c/a\u003e â€“ \u003cem\u003eCausality from a Distributional Robustness Point of View\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://academic.oup.com/jrsssb/article/83/2/215/7056043\"\u003eRothenhÃ¤usler et al. (2021)\u003c/a\u003e â€“ \u003cem\u003eAnchor Regression: Heterogeneous Data Meet Causality\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9476906\"\u003eChristiansen et al. (2022)\u003c/a\u003e â€“ \u003cem\u003eA Causal Framework for Distribution Generalization\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://academic.oup.com/ectj/article/25/2/404/6380481\"\u003eJakobsen et al. (2022)\u003c/a\u003e â€“ \u003cem\u003eDistributional Robustness of K-Class Estimators and the PULSE\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis project offers a concise introduction to this learning framework and highlights our contribution to the field. For a quick primer, check out the following:\u003c/p\u003e\n\u003cp\u003eðŸ‘‰ \u003ca href=\"../../courses/anchor_mva_robustness\"\u003eIntroduction to out-of-distribution generalization from a causal perspective\u003c/a\u003e\u003c/p\u003e\n","description":"","image":"/images/projects/anchor.png","permalink":"https://hugo-profile.netlify.app/projects/anchor_mva/","title":"Out-of-distribution Generalisation via Diluted Causality"},{"content":"\u003cbase target=\"_blank\"\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eDuring my end-of-study internship at the LOCEAN-IPSL laboratory, I have been working on coupled ocean-atmosphere models calibration.\u003c/p\u003e\n\u003cp\u003eThe ccentral goal of the the project was to better understand if History Matching technique was adapated for calibrating coupled ocean-atmosphere models. Using a simplified model, the two layers Lorenz-96 we demonstrated that History Matching method effectively constrained the model\u0026rsquo;s parameter search space. Notably, this was validated across scenarios resembling AMIP (Atmospheric Model Intercomparison Project) and OMIP (Ocean Model Intercomparison Project) experiments.\u003c/p\u003e\n\u003cp\u003eWe also experimented the use of machine learning alternative to gaussian processes as emulators. We carried a comparative study of Ramdom Forest and Bayesian Neural Networks with the commonly used gaussian process. While these methods do not seem to show strong performance improvement in the convergence speed to the real parameters, they seem to perform well and some further investigation might be promising.\u003c/p\u003e\n\u003cp\u003eWe finally compared two approaches to reduce the metric space: Principal Component Analysis (as this is usually used) and autoencoders. We show that by retaining more information about the metrics in a smaller subspace, the former improve the algorithm convergence speed and accuracy.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/project_reports/Rapport_Stage.pdf\"\u003e\u003cem\u003e\u003cstrong\u003eRead full report\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n","description":"","image":"/images/projects/HM.png","permalink":"https://hugo-profile.netlify.app/projects/history_matching/","title":"History Matching for climate model tuning - experiments on the Lorenz96 toy model"},{"content":"\u003cp\u003eThis course notes are in preparation and should be released around mid-May.\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/intro_objective_bayes/","title":"01 - An introduction to objective bayesianism with climate science examples"},{"content":"\u003cp\u003eThis course notes are in preparation and should be released around mid-May.\u003c/p\u003e\n\u003c!-- \n## Introduction\n\n  * History of ensemble forecasting (Roots of Ensemble Forecasting, J. M. Lewis 2005)\n\n\n## Chaos theory\n\nLyapunov exponent $\\lambda$ measure the sensitivity to initial conditions\n\n$$\n| \\delta Z(t) | = e^{\\lambda t} |\\delta Z_0 |.\n$$\n\nThere could be multiple lyapunov exponent and we usually refer to the largest one, called MLE.\n\n* Periodic behavior of chaotic system must be repelling not attracting\n* Topological mixing: any given regions will eventually overlapp with any other given region\n## Types of uncertainties\n\n## Ensemble forecasting\n\nRetrospective\n\n## From weather to climate\n\n## Counterfactuals\n\n## Predictability\n\n**Palmer 2006:**\n* We should think of weather and climate prediction in terms of equations whose basic prognostic variables are probability densities $\\rho(X, t)$ where $X$ denotes some climatic variables and $t$ denotes time.\n* $\\rho(X, t)dV$ denotes the probability that at time $t$, $X$ true value lies in some volume $dV$.\n* **Fokkerâ€“Planck equation:**  partial differential equation that describes the time evolution of the probability density function of the velocity of a particle under the influence of drag forces and random forces, as in Brownian motion \n* **Liouville equation:**  describes the time evolution of the phase space distribution function (fundamental equation of statistical mechanics).\n\n* Fokker-Planck and Liouville equations describe the evolution of the climate system and are practically solved using ensemble techniques.\n* Prior estimate $\\rho_C(X)$ climatological density of $X$.\nWeather or not $X$ is predictable depends wether $\\rho(X, t)$ is sufficiently different from $\\rho_C(X)$, i.e. it is predictable if they are different\n\n\u003cdiv style=\"display: flex; align-items: center;\"\u003e    \n    \u003cimg src=\"../Images/forecast_distributions_palmer_2006.png\" alt=\"Image\" style=\"width: 300px; height: auto; margin-right: 40px;\"\u003e\n    \u003cp\u003e\n    If the distributions are very different the weather forecast could be used by decision-makers. The way to compute this divergence is not clear and would depend on the specific decision making. Where one is mainly interested about extreme events then measuring the distance between the probabilities after a critical point might be prefered to an overall distance measure. This might for example be the case in figure 1.1.\n    \u003c/p\u003e\n    \n\u003c/div\u003e\n\n* $\\dot{X} = F(X)$ might be nonlinear and the Jacobian $dF/dX$ depends linearly (at least) on the state $X$\n$$\n\\frac{d \\delta X}{dt} = \\frac{dF}{dX}\\delta X.\n$$\n\nThe tangent propagator \n$$\nM(t, t_0) = \\text{exp}\\int_{t_0}^t  \\frac{dF}{dX}\\delta t'\n$$\ndepends on the nonlinear trajectory $X(t)$ about which the linearisation is performed. Hence the evolved perturbation becomes\n$$\n\\delta X(t) = M(t, t_0)\\delta X(t_0).\n$$\n\n\u003cdiv style=\"display: flex; align-items: center;\"\u003e    \n    \u003cimg src=\"../Images/Lorenz69_Palmer_2006.png\" alt=\"Image\" style=\"width: 300px; height: auto; margin-right: 40px;\"\u003e\n    \u003cp\u003e\n    Assume that all points on the left of the attractor are frosty and the ones on the right side are frost-free. On the basis of the top left forecast all next weather are gonna be frosty. On the basis of the bottom forecast it is impossible to say if it is gonna be frosty or not.\n    \u003c/p\u003e\n    \n\u003c/div\u003e\n\nThe solution to Liouville equation which formally describes the evolution of $\\rho(X, t)$ arising from initial errors only can be written using the tangent operator, ie\n$$\n\\rho(X, t) = \\rho(X', t)/det(M(t, t_0))\n$$\n\nwhere $X'$ corresponds to the initial state which evolves into state $X$ at tie $t$.\n\n**Types of uncertainties:**\n* Uncertainty in observations used to define initial state\n  * \n* Uncertainty in the model used to assimilate the observations and make the forecasts\n* Uncertainty in 'external' parameters:\n  * Content of aerosols, change in CO2 emissions\n  * Should we add random volcanoes into simulations? How would this modify the distribution estimation?\n  * \n\n#### Initial uncertainty\n\nData assimilation:\n$$\nJ(X) = \\frac{1}{2}(X - X_b)^\\top B^{-1}(X - X_b) + \\frac{1}{2}(HX - Y)^\\top O^{-1}(HX - Y)\n$$\n\nwhere $X_b$ is the background state, $B$ and $O$ are covariance matrices for the pdf of background error and observation error, $H$ is the **observation operator** and $Y$ the vector of available observations. (see [Courtier et al., 1998](https://rmets.onlinelibrary.wiley.com/doi/epdf/10.1002/qj.49712455002)). The hessian \n$$\n\\nabla^2_X J = B^{-1} + H^\\top O^{-1}H\n$$\ndefines the inverse analysis error covariance matrix.\n\n\u003cdiv style=\"display: flex; align-items: center;\"\u003e    \n    \u003cimg src=\"../Images/Isopleth_Palmer_2006.png\" alt=\"Image\" style=\"width: 300px; height: auto; margin-right: 40px;\"\u003e\n    \u003cp\u003e\n    We can see how an isopleth of the covariance matrix of errors evolves under the action of the tangent propagator $M$. The vector pointing along the major axis at forecast time corresponds to the leading eigenvector of the forecast error covariance matrix. Its preimage at initial time corresponds to the leading eigenvector of MtM.\n    \u003c/p\u003e\n    \n\u003c/div\u003e\n\nGiven pdfs of uncertainty based on def of $J(X)$ we can in principle perform Monte Carlo sampling of the Hessian-based initial pdf and produce ensemble forecast system based on initial sampling. There is three good reasons not to do that:\n* **Curse of dimensionality:** The sate space of a weather prediction model is too big (order $10^7$) making the process too computationally intensive. See [Lorenz 1965](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2153-3490.1965.tb01424.x) and its 28 variables model.\n* **Other uncertainties:** In practice, initial conditions uncertainties are not the only uncertainties. But the remaining uncertainties might not be well defined as part of *unknown unknowns*. Some process are well captured by simulations at the given grid size (laminar flow) but some others not at all (turbulent flow).\n* **Overconservativness:** might be prefered for very risky events. Thus one would need to sample perturbations that are likely to have significant impacts on the forecast.\n\nFor these reasons, together with the fact that the 20 to 30 first eigenvalues of $M^\\top M$ are of greater scales than the remaining ones, the initial perturbations of the ECMWF ensemble prediction system are based on the leading eigen vectors of $M^\\top M$. \n\n#### Model uncertainty\n\n* There is no underlying theory which allows us to estimate the statistical uncertainty when integratingequations of climate on a computer. \n* **Parametetrisation:** is the process of approximating the effects of unreolved processes on the resolved scales. \n* **Hierarchical uncertainties:**\n  * Multimodel ensemble (DEMETER)\n  * multiparametrisation ensemble (Meteorological Service of Canada)\n  * multiparameter ensemble (perturbation of $\\alpha$) (climateprediction.net ensemble system)\n\nThis hierarchical representation is rather pragmatic than theoretically grounded approach (and not complete).\n\n**Souldn't we use the MAP instead of the mean state???**\n\nLet's note $\\rho_m(X)$  be the representation of unresolved scales where $X$  is some resolved-scale variable. Let consider an ensemble prediction system where the grid-box mean variable is $X_0$ across all members of the ensemble. Assuming that instead of using he deterministic subgrid parameterisation $P(X_0, \\alpha)$ across all members we force the ensemble prediction system by randomly sampling $\\rho_m(X_0)$. Then the ensemble-mean would evolve differently because of nonlinearity. **Making it as a question.** Why?\n\n* We move from an arbitrary ensemble of multimodels where parametrisations difference does not represent anything meaningful to one where each ensemble member is equipped with a possible realisation of subgrid process.\n* **Reasons why it should be better (stochastic dynamic vs multimodel):**\n  * More accurate representation of model uncertainties\n  * Reduction of model systematic errors\n  * More accurate internal variability representation (important for **climate change detection**)\n\n  We can write the equations of motion of the climate/weather prediction model as\n$$\n\\dot{X} = F(X) + P + e\n$$\n\nwhere $e = \\epsilon P$ and $P$ denotes the conventional parametrisation term and $\\epsilon$ \n\n#### Types of forecast\n\n**Extended range** (monthly time scales): Used originally to find which period were predictable (more than $P_W$). In early times, the methods fro initial condition perturbations were simple:  adding random noise on initial conditions or using time lagged techniques. In 2004: singular vector ensemble initial conditions perturbations (see [Vitart 2004](https://journals.ametsoc.org/view/journals/mwre/132/12/mwr2826.1.xml)).\n\n**Medium range forecast:** Using both singular values initial perturbation and stochatsic physics.\n* [Lothar cyclone](https://en.wikipedia.org/wiki/Cyclone_Lothar) in december 1999 was exceptionally unpredictable and even 42 hours lead time there is a considerable spread in the ensemble. The best guidance deterministic forecast only predicts a weak trough in surface pressure. \n* However a couple of members show a strong vortex over France. The ensemble was able to predict something that a deterministic forecast could not. \n\n**Seasonal and decadal predictions:** Interaction between atmosphere and slower components such as oceans and land surface.\n* Predictability of ENSO are predictable seasons ahead of time using intermediate-complexity coupled ocean-atmosphere of the tropical Pacific.\n* Using inital conditions perturbation of both atospheric and ocean state.\n* Use of multimodel ensembles (DEMETER)\n\n\n**Climate change:**\n\n**Short range forecasting:** For a long time, weather forcasting was seen as a deterministic task up to 2 days. Nowadays, falshfloods and other similar events \n* Ensemble boundary conditions\n* initial perturbations\n\n**Underdispersivity and unknown unknowns**\n\n#### Uncertain forecast from deterministic dynamics\n\n* Lorenz system:\n\t* Each dote is a weather of possible climate (attractor)\n\t* Weather can be seen as a realisation of the irreducible uncertainty of the chaotic climate system\n\t* Predictability of Lorenz (like climate) is very state dependent\n\t* Phase space\n\t* If proba of initial conditions represents correctly uncertainty and model correct then we get correct estimate of forecast uncertainty\n\t* Stochastic dynamic predictions (Epstein 1969)\n\t* Liouville Ã©quation\n\n$$ \n\\frac{\\partial \\phi}{\\partial t} +  \\nabla (\\dot{X} \\phi) = 0\n$$\n\nIs familiar to continuity (ie conservation) Ã©quation for mass. Here $\\phi$ is the uncertainty distrib and $\\dot{X}$ the total derivatives wrt time of the prognostic variables defining the coordinate axes of the state space.\n\t* The total proba of any system is by def 1\n\t* The governing physical dynamics are contained on $\\dot{X}$, known as tendencies. Integration of this equation is deterministic (there are no Radom terms introduced on the right hand sides of the dynamical tendencies)\n\t* Stochastic diffusion forcings and jump processes are integrated threw Chapman-Kolmogorov equation\n\t* But direct intergation too computationally expensive\n\n\n* **personal questions:**\n  * What makes that sometimes errors of dynamical system grow exponentially and sometime not? \n  * Also how can some part of the system (eg mean state) do not have an error growing exponentially? How can we get a linear response? To some perturbations and a non linear one to others?  --\u003e","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/intro_ensemble_forecasting/","title":"03 - An introduction to ensemble forecasting"},{"content":"\u003cp\u003eThis course notes are in preparation and should be released around mid-May.\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/intro_validation_probabilistic_forecasting/","title":"04 - An introduction to the validation of probabilistic forecasting"},{"content":"\u003cp\u003eWe here summarise the work of \u003ca href=\"https://ieeexplore.ieee.org/document/8439889\"\u003eMeinshausen (2018)\u003c/a\u003e. We recommend reading the original paper for a more complete overview of the ideas sketched here.\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe most common learning paradigm in statistical learning is arguably Empirical Risk Minimization (ERM), which can be written as:\u003c/p\u003e\n\u003cp\u003e$$\n\\arg\\min_{\\theta} \\mathbb{E}[\\ell(Y, f_{\\theta}(X))]\n$$\u003c/p\u003e\n\u003cp\u003ewhere $X \\in \\mathbb{R}^d$ is a set of covariates and $Y \\in \\mathbb{R}$ is a target variable. We aim to find the parameters $\\theta$, which parametrize the function $f_\\theta: \\mathbb{R}^d \\rightarrow \\mathbb{R}$, by minimizing the expected value of the loss $\\ell: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$.\u003c/p\u003e\n\u003cp\u003eWhile this is very useful when the goal is to predict outcomes for new samples $X$ drawn from the same distribution as the training data, it can fail dramatically when the test distribution shifts â€” a common scenario in real-world applications. In this context, a more robust learning framework is the minimization of the worst-case risk, where the distribution $Q$ of $(X, Y)$ is chosen from a set of possible distributions $\\mathbb{Q}$. This can be formally written as:\u003c/p\u003e\n\u003cp\u003e$$\n\\arg\\min_{\\theta} \\sup_{Q \\in \\mathbb{Q}} \\mathbb{E}[\\ell(Y, f_{\\theta}(X))]\n$$\u003c/p\u003e\n\u003cp\u003eThe class $\\mathbb{Q}$, often referred to as the \u003cem\u003euncertainty set\u003c/em\u003e, can be constrained using distributional metrics such as $f$-divergence (see \u003ca href=\"https://papers.nips.cc/paper_files/paper/2016/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf\"\u003eNamkoong (2016)\u003c/a\u003e) or the Wasserstein distance (see \u003ca href=\"https://link.springer.com/article/10.1007/s10107-017-1172-1\"\u003eEsfahani (2015)\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eIn this work, we consider a set of distributions $\\mathbb{Q}$ arising from interventions on covariates. We will see that allowing different sets of interventions leads to different types of robustness and generalization guarantees. The next section introduces key concepts from causality to help define intervention-based distributions.\u003c/p\u003e\n\u003ch2 id=\"the-causal-inference-paradigm\"\u003eThe Causal Inference Paradigm\u003c/h2\u003e\n\u003ch3 id=\"modularity-assumption\"\u003eModularity Assumption\u003c/h3\u003e\n\u003cp\u003eLetâ€™s begin with a simple example. Suppose we are given two variables, temperature $T$ and altitude $A$, along with their joint distribution $p(T, A)$. Statistically, this distribution can be factored in two ways:\u003c/p\u003e\n\u003cp\u003e$$\np(A, T) = p(A \\mid T)p(T)\n$$\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003e$$\np(A, T) = p(T \\mid A)p(A)\n$$\u003c/p\u003e\n\u003cp\u003eThe second factorization is considered \u003cem\u003ecausal\u003c/em\u003e because $p(T \\mid A)$ reflects a physical mechanism linking altitude to temperature, up to some noise. This mechanism is said to be \u003cem\u003emodular\u003c/em\u003e, meaning that if we intervene by changing altitude, the mechanism remains stable.\u003c/p\u003e\n\u003cp\u003eIn contrast, the first factorization is \u003cem\u003enon-causal\u003c/em\u003e, as $p(A \\mid T)$ does not represent a stable mechanism. There\u0026rsquo;s no reliable way to model how altitude would respond to changes in temperature.\u003c/p\u003e\n\u003cp\u003eIf a joint distribution is entailed by a directed acyclic graph (DAG), the graph is called \u003cem\u003ecausal\u003c/em\u003e if it respects modularity. That is, manipulating one variable does not alter the conditional distributions of the others. This becomes more intuitive under a \u003cstrong\u003eStructural Causal Model\u003c/strong\u003e (SCM), where each variable is defined by a function of its parents. Formally, an SCM $\\mathcal{C}$ consists of structural assignments:\u003c/p\u003e\n\u003cp\u003e$$\nX_i := f_i(PA_i, N_i), \\quad i = 1, \\dots, n\n$$\u003c/p\u003e\n\u003cp\u003ewhere $PA_i$ are the parents of $X_i$, $N_i$ is a noise term, and $f_i$ is a stable (modular) function. The modularity assumption implies that interventions change only the relevant functions, leaving others untouched.\u003c/p\u003e\n\u003ch3 id=\"interventions\"\u003eInterventions\u003c/h3\u003e\n\u003cp\u003eSo far, weâ€™ve used the idea of \u003cem\u003emanipulation\u003c/em\u003e informally. In causal inference, we formalize it using the concept of \u003cem\u003eintervention\u003c/em\u003e. Given a set of variables $X$ governed by an SCM, a (strong) intervention sets a variable to a fixed value.\u003c/p\u003e\n\u003cp\u003eReturning to the altitude and temperature example, assume their relationship is captured by the SCM:\u003c/p\u003e\n\u003cp\u003e$$\nT = f(A, N)\n$$\u003c/p\u003e\n\u003cp\u003ewhere $f$ is a deterministic, modular function and $N$ is some noise. A strong intervention sets the random variable $A$ to a specific value $a$, yielding the interventional distribution:\u003c/p\u003e\n\u003cp\u003e$$\nT^{do(A=a)} = f(a, N)\n$$\u003c/p\u003e\n\u003cp\u003eNote that this distribution remains random due to the noise $N$.\u003c/p\u003e\n\u003cp\u003eMore generally, for an SCM $\\mathcal{C}$, we denote the distribution under a hard intervention as $P_X^{\\mathcal{C}, do(X_i := x_i)}$, or simply $P_X^{do(X_i := x_i)}$.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003ePhilosophical note\u003c/strong\u003e: This idea aligns with \u003cem\u003eWoodwardâ€™s manipulation theory of causation\u003c/em\u003eâ€”which understands causation as a systemâ€™s response to manipulation. A manipulation (or intervention) breaks a variableâ€™s usual causal influences and sets it to a new value. This notion of intervention is central to the manipulationist theory of causation. For an introduction to this perspective, see \u003ca href=\"https://plato.stanford.edu/entries/causation-mani/\"\u003eWoodward (2001)\u003c/a\u003e.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"distribution-generalization-via-causal-inference\"\u003eDistribution Generalization via Causal Inference\u003c/h2\u003e\n\u003cp\u003eConsider the case where $P_{X, Y}$ is entailed by the following linear SCM:\u003c/p\u003e\n\u003cp\u003e$$\nX = X\\mathbf{B} + N \\in \\mathbb{R}^d\n$$\u003c/p\u003e\n\u003cp\u003e$$\nY = X\\mathbf{b} + N_y \\in \\mathbb{R}\n$$\u003c/p\u003e\n\u003cp\u003eWe denote the interventional distribution obtained by setting some component of $X$ to a specific value as $P_{X, Y}^{do(X_i := x_i)}$.\u003c/p\u003e\n\u003cp\u003eDefine $\\mathbb{Q}$ as the set of all such interventional distributions:\u003c/p\u003e\n\u003cp\u003e$$\n\\mathbb{Q} := {P_{X, Y}^{do(X := x)} \\mid x \\in \\mathcal{X} \\subseteq \\mathbb{R}^d}\n$$\u003c/p\u003e\n\u003cp\u003eIt can be shown that for the squared loss $\\ell(Y, f_\\theta(X)) := (Y - f_\\theta(X))^2$, we have the equivalence:\u003c/p\u003e\n\u003cp\u003e$$\n\\theta^{\\text{causal}} = \\arg\\min_{\\theta} \\sup_{Q \\in \\mathbb{Q}} \\mathbb{E}[\\ell(Y, f_\\theta(X))]\n$$\u003c/p\u003e\n\u003cp\u003ewhere $f_{\\theta^{\\text{causal}}}$ is the structural equation from the SCM. The intuition is straightforward:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{cases}\n\\infty \u0026amp; \\text{if } \\theta \\ne \\theta^{\\text{causal}} \\\n\\operatorname{Var}(N_y) \u0026amp; \\text{if } \\theta = \\theta^{\\text{causal}}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003eIf $\\theta \\ne \\theta^{\\text{causal}}$, there exists some intervention that causes the loss to diverge. In contrast, if $\\theta = \\theta^{\\text{causal}}$, the loss becomes independent of $X$ and equals the variance of the noise.\u003c/p\u003e\n\u003ch2 id=\"discussion\"\u003eDiscussion\u003c/h2\u003e\n\u003cp\u003eWe proposed a framework showing that, in the linear case and for the squared loss, worst-case risk minimization for out-of-distribution generalization is equivalent to recovering the causal parameters of an SCMâ€”when the uncertainty set $\\mathbb{Q}$ includes all possible interventions on $X$.\u003c/p\u003e\n\u003cp\u003eIn the next chapter, we explore how this approach might be overly conservative. Restricting $\\mathbb{Q}$ to a specific subset of interventions can sometimes yield better generalization. This direction builds on the work of \u003ca href=\"https://academic.oup.com/jrsssb/article/83/2/215/7056043\"\u003eRothenhÃ¤usler et al. (2021)\u003c/a\u003e and the idea of \u003cstrong\u003eanchor regression\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eðŸ‘‰ \u003ca href=\"../anchor_regression\"\u003eAnchor regression as a diluted form of causality\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/anchor_mva_robustness/","title":"01 - From causal inference to out-of-distribution generalization simulation"},{"content":"\u003cp\u003eThis course notes are in preparation and should be released around mid-May.\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/intro_bayesian_uq_of_computer_experiments/","title":"02 - An introduction to uncertainty quantification of physical simulation"},{"content":"\u003cp\u003eWe here summarise the work of \u003ca href=\"https://academic.oup.com/jrsssb/article/83/2/215/7056043\"\u003eRothenhÃ¤usler et al. (2021)\u003c/a\u003e. We recommend reading the original paper for a more complete overview of the ideas sketched here.\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWe are still interested in worst-case risk minimisation as described in the previous chapter. Given $X \\in \\mathbb{R}^d$ and $Y \\in \\mathbb{R}$, we aim to minimise\u003c/p\u003e\n\u003cp\u003e$$\n\\arg\\min_{\\mathbf{b}} \\sup_{Q \\in \\mathbb{Q}} \\mathbb{E}[(Y - X\\mathbf{b})^2].\n$$\u003c/p\u003e\n\u003cp\u003eWe use the squared loss and again assume a linear SCM, but now with a slightly more elaborate structure. We assume that a part of the covariates are known to be exogenous. We call these \u003cem\u003eanchor variables\u003c/em\u003e, denoted $A$. Anchor variables aim to generalise the framework of instrumental variables (IV) regression, which assumes the presence of an exogenous variable that affects $Y$ only through $X$ (exclusion restriction). The anchor framework relaxes this exclusion restriction by only assuming exogeneity of $A$.\u003c/p\u003e\n\u003cp\u003eMore concretely, the distribution of $(A, X, Y)$ is entailed by the following SCM:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix} X \\ Y \\ H \\end{pmatrix} = \\mathbf{B} \\begin{pmatrix} X \\ Y \\ H \\end{pmatrix} + \\varepsilon + \\mathbf{M} A,\n$$\u003c/p\u003e\n\u003cp\u003ewhere $H$ represents hidden confounders.\u003c/p\u003e\n\u003cp\u003eAssuming the graph is acyclic, the matrix $(I - \\mathbf{B})$ is invertible, and the SCM can be written as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix} X \\ Y \\ H \\end{pmatrix} = (I - \\mathbf{B})^{-1}(\\varepsilon + \\mathbf{M} A).\n$$\u003c/p\u003e\n\u003cp\u003eIn this framework, we allow interventions only on the anchor variables $A$. The intervened SCM is:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix} X \\ Y \\ H \\end{pmatrix} = (I - \\mathbf{B})^{-1}(\\varepsilon + \\nu),\n$$\u003c/p\u003e\n\u003cp\u003ewhere $\\nu$ is the hard intervention. We denote by $P^\\nu$ the interventional distribution, and $\\mathbb{E}_\\nu$ its corresponding expectation (also referred to as the test distribution). The training distribution is denoted by $P$.\u003c/p\u003e\n\u003cp\u003eBecause of the presence of hidden confounders $H$, the causal parameters are non-identifiable. Thus, learning a causal model that remains stable under all possible interventions is impossible (see previous chapter). Interestingly, \u003ca href=\"https://academic.oup.com/jrsssb/article/83/2/215/7056043\"\u003eRothenhÃ¤usler et al. (2021)\u003c/a\u003e also show that causal parameters might even be sub-optimal for prediction in the presence of hidden confounders.\u003c/p\u003e\n\u003ch2 id=\"bounded-interventions\"\u003eBounded Interventions\u003c/h2\u003e\n\u003cp\u003eThe core idea of anchor regression is to restrict the strength of interventions on $A$. This is formalised as:\u003c/p\u003e\n\u003cp\u003e$$\n\\mathbb{Q}^{\\text{anchor}} = { P^\\nu \\mid \\nu \\nu^\\top \\preceq \\gamma , \\mathbb{E}_P[AA^\\top] }.\n$$\u003c/p\u003e\n\u003cp\u003eBy constraining the set of possible interventions, we may obtain better generalisation performance. In practice, interventions are typically bounded, making this assumption reasonable.\u003c/p\u003e\n\u003ch2 id=\"robustness-under-diluted-causality\"\u003eRobustness Under Diluted Causality\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://academic.oup.com/jrsssb/article/83/2/215/7056043\"\u003eRothenhÃ¤usler et al. (2021)\u003c/a\u003e show that the worst-case risk over the set $\\mathbb{Q}^{\\text{anchor}}$ takes a particularly simple form as a causal regularisation of Empirical Risk Minimisation (ERM):\u003c/p\u003e\n\u003cp\u003e$$\n\\sup_{P^\\nu \\in \\mathbb{Q}^{\\text{anchor}}} \\mathbb{E}_\\nu[(Y - X\\mathbf{b})^2] = \\mathbb{E}_P[(Y - X\\mathbf{b})^2] + (\\gamma - 1) , \\mathbb{E}_P\\left[(P_A(Y - X\\mathbf{b}))^2\\right],\n$$\u003c/p\u003e\n\u003cp\u003ewhere $P_A(\\cdot) = \\mathbb{E}[\\cdot \\mid A]$ denotes the linear projection onto the space spanned by $A$. The parameter $\\gamma$ captures the strength of the interventions in $\\mathbb{Q}^{\\text{anchor}}$ to which we want robustness.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe first term on the right-hand side is the standard ERM loss.\u003c/li\u003e\n\u003cli\u003eThe second term is a causal regularisation term.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis second term is equivalent to the two-stage least squares formulation of instrumental variables. Special cases include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\gamma = 1$: standard ERM,\u003c/li\u003e\n\u003cli\u003e$\\gamma = \\infty$: IV regression,\u003c/li\u003e\n\u003cli\u003e$\\gamma = 0$: partialling-out regression, another causal inference technique.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"discussion\"\u003eDiscussion\u003c/h2\u003e\n\u003cp\u003eWe have shown how restricting the set of potential interventions may lead to better generalisation under bounded interventions. So far, this framework has considered only the quadratic loss. In the next chapter, we will see how this regularisation idea can be extended to a wider class of algorithms.\u003c/p\u003e\n\u003cp\u003eðŸ‘‰ \u003ca href=\"../anchor_mva\"\u003eAnchor regression generalisation for multivariate algorithms\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/anchor_regression/","title":"02 - Anchor Regression as a diluted form of causality"},{"content":"\u003cp\u003eThis chapter will be available soon.\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/anchor_mva_climate/","title":"03 - Anchor regression generalisation for multivariate algorithms"},{"content":"\u003cp\u003eWe here summarise the work of \u003ca href=\"https://arxiv.org/abs/2403.01865\"\u003eDurand et al. (2025)\u003c/a\u003e. We recommend reading the original paper for a more complete overview of the ideas sketched here.\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWe continue working within the worst-case risk framework, but now consider a general loss function $\\mathcal{L}(X, Y; \\theta)$, whose form will be clarified later. The goal becomes solving:\u003c/p\u003e\n\u003cp\u003e$$\n\\arg\\min_{\\theta} \\sup_{Q \\in \\mathbb{Q}} \\mathbb{E}[\\mathcal{L}(X, Y; \\theta)].\n$$\u003c/p\u003e\n\u003cp\u003eAssume the observed variables $(X, Y)$ follow the linear SCM from \u003ca href=\"https://academic.oup.com/jrsssb/article/83/2/215/7056043\"\u003eRothenhÃ¤usler et al. (2021)\u003c/a\u003e:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix} X \\ Y \\ H \\end{pmatrix} = \\mathbf{B} \\begin{pmatrix} X \\ Y \\ H \\end{pmatrix} + \\varepsilon + \\mathbf{M} A.\n$$\u003c/p\u003e\n\u003cp\u003eGiven acyclicity of the graph, we have:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix} X \\ Y \\ H \\end{pmatrix} = (I - \\mathbf{B})^{-1}(\\varepsilon + \\mathbf{M} A),\n$$\u003c/p\u003e\n\u003cp\u003eor more simply, for some matrix $\\mathbf{D}$,\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix} X \\ Y \\end{pmatrix} = \\mathbf{D}(\\varepsilon + \\mathbf{M} A).\n$$\u003c/p\u003e\n\u003ch2 id=\"bounding-intervention-covariance\"\u003eBounding Intervention Covariance\u003c/h2\u003e\n\u003cp\u003eFrom this, we can write the covariance matrix of $\\begin{pmatrix} X \\ Y \\end{pmatrix}$ as:\u003c/p\u003e\n\u003cp\u003e$$\n\\Sigma_{XY} = \\mathbf{D}\\Sigma_{\\varepsilon}\\mathbf{D}^\\top + \\mathbf{D}\\mathbf{M}\\Sigma_{A}\\mathbf{M}^\\top \\mathbf{D}^\\top,\n$$\u003c/p\u003e\n\u003cp\u003eassuming $\\varepsilon$ and $A$ are independent. Similarly, the covariance under intervention becomes:\u003c/p\u003e\n\u003cp\u003e$$\n\\Sigma_{XY}^{do(A := \\nu)} = \\mathbf{D}\\Sigma_{\\varepsilon}\\mathbf{D}^\\top + \\mathbf{D}\\mathbf{M}\\Sigma_{\\nu}\\mathbf{M}^\\top \\mathbf{D}^\\top.\n$$\u003c/p\u003e\n\u003cp\u003eWith the set of bounded interventions:\u003c/p\u003e\n\u003cp\u003e$$\n\\mathbb{Q}^{\\text{anchor}} = { P^\\nu \\mid \\nu \\nu^\\top \\preceq \\gamma , \\mathbb{E}_P[AA^\\top] },\n$$\u003c/p\u003e\n\u003cp\u003ewe obtain for all $P \\in \\mathbb{Q}^{\\text{anchor}}$:\u003c/p\u003e\n\u003cp\u003e$$\n\\Sigma_{XY}^{do(A := \\nu)} \\preceq \\mathbf{D}\\Sigma_{\\varepsilon}\\mathbf{D}^\\top + \\gamma \\mathbf{D}\\mathbf{M} \\Sigma_{A} \\mathbf{M}^\\top \\mathbf{D}^\\top.\n$$\u003c/p\u003e\n\u003cp\u003eHence, we can bound $\\Sigma_{XY}$ across this intervention family, which is valuable since many methods depend on $\\Sigma_{XY}$.\u003c/p\u003e\n\u003ch2 id=\"anchor-compatible-losses\"\u003eAnchor-Compatible Losses\u003c/h2\u003e\n\u003cp\u003eWe define a class of losses suitable for anchor regularisation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition (Anchor-compatible loss):\u003c/strong\u003e\u003cbr\u003e\n\u003cem\u003eA loss function $\\mathcal{L}(X, Y; \\theta)$ is anchor-compatible if it can be written as:\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e$$\n\\mathcal{L}(X, Y; \\theta) = f_{\\theta}(C_{XY}),\n$$\u003c/p\u003e\n\u003cp\u003e\u003cem\u003ewhere $f_\\theta : \\mathbb{R}^{d \\times p} \\to \\mathbb{R}$ is a linear map, and $C_{XY} = \\begin{pmatrix} X \\ Y \\end{pmatrix} \\otimes \\begin{pmatrix} X \\ Y \\end{pmatrix}$.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"out-of-distribution-generalisation\"\u003eOut-of-Distribution Generalisation\u003c/h2\u003e\n\u003cp\u003eFor anchor-compatible losses, we have the following guarantee:\u003c/p\u003e\n\u003cp\u003eLet $(X, Y, H)$ follow the SCM above and let $\\mathcal{L}(X, Y; \\theta)$ be anchor-compatible. Then, for any $\\theta$ and $\\gamma \u0026gt; 0$:\u003c/p\u003e\n\u003cp\u003e$$\n\\sup_{P \\in \\mathbb{Q}^{\\text{anchor}}} \\mathbb{E}_P[\\mathcal{L}(X, Y; \\theta)] = f _\\theta(\\Sigma _{XY}) + (\\gamma - 1) f _\\theta(\\Sigma _{XY|A}).\n$$\u003c/p\u003e\n\u003ch2 id=\"robustness-of-multivariate-analysis-algorithms\"\u003eRobustness of Multivariate Analysis Algorithms\u003c/h2\u003e\n\u003cp\u003eThis framework enables robustness for popular multivariate methods such as Multiple Linear Regression (MLR), Orthogonal PLS (OPLS), Reduced Rank Regression (RRR), and Partial Least Squares (PLS). However, some algorithms like Canonical Correlation Analysis (CCA) are not compatible with anchor regularisation due to the lack of theoretical guarantees.\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eMethod\u003c/th\u003e\n          \u003cth\u003eLoss\u003c/th\u003e\n          \u003cth\u003eConstraints\u003c/th\u003e\n          \u003cth\u003eCompatible\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eMLR\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e$| Y - \\mathbf{W}^T X |_F^2$\u003c/td\u003e\n          \u003ctd\u003eâ€“\u003c/td\u003e\n          \u003ctd\u003eâœ”\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eOPLS\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e$| Y - \\mathbf{U} \\mathbf{V}^T X |_F^2$\u003c/td\u003e\n          \u003ctd\u003e$\\mathbf{U}^T \\mathbf{U} = \\mathbf{I}$\u003c/td\u003e\n          \u003ctd\u003eâœ”\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eRRR\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e$| Y - \\mathbf{W} X |_F^2$\u003c/td\u003e\n          \u003ctd\u003e$\\text{rank}(\\mathbf{W}) = \\rho$\u003c/td\u003e\n          \u003ctd\u003eâœ”\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003ePLS\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e$-\\text{tr}( \\mathbf{W_x}^T X^T Y \\mathbf{W_y} )$\u003c/td\u003e\n          \u003ctd\u003e$\\mathbf{W_x}^T \\mathbf{W_x} = \\mathbf{I}, \\mathbf{W_y}^T \\mathbf{W_y} = \\mathbf{I}$\u003c/td\u003e\n          \u003ctd\u003eâœ”\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eCCA\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e$-\\text{tr}( \\mathbf{W_x}^T X^T Y \\mathbf{W_y} )$\u003c/td\u003e\n          \u003ctd\u003e$\\mathbf{W_x}^T C_X \\mathbf{W_x} = \\mathbf{I}, \\mathbf{W_y}^T C_Y \\mathbf{W_y} = \\mathbf{I}$\u003c/td\u003e\n          \u003ctd\u003eâœ˜\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"discussion\"\u003eDiscussion\u003c/h2\u003e\n\u003cp\u003eWe showed that anchor regularisation applies to a broader class of loss functions beyond least squares. Any loss expressible as a linear map of the joint covariance $\\Sigma_{XY}$ can be anchor-regularised via a simple causal term. This extends OOD generalisation to many standard multivariate learning algorithms.\u003c/p\u003e\n\u003c!-- Next, we explore applications to climate change attribution.\n\n[Next chapter: Robust estimation of forced climate response](../anchor_mva_climate) --\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/anchor_mva/","title":"03 - Generalizing Anchor Regression to Multivariate Algorithms"},{"content":"\u003cp\u003eThis course notes are in preparation and should be released around mid-May.\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/intro_detection_and_attribution/","title":"05 - An introduction to detection and attribution of climate change"},{"content":"\u003cp\u003eThis course notes are in preparation and should be released around mid-May.\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/courses/intro_extreme_events_attribution/","title":"06 - An introduction to extreme events attribution"},{"content":"\u003cbase target=\"_blank\"\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eLynch Syndrome is associated with a significantly elevated risk of developing colorectal cancer (CRC). However, this risk can be mitigated through early detection and intervention. A pivotal diagnostic method involves identifying lesions in enterocytes using immunohistochemistry on colon tissue samples. A deficient Mismatch Repair (MMR) crypt, termed Adenomatous Crypt Foci (ACF), indicates Lynch Syndrome.\u003c/p\u003e\n\u003cp\u003eDistinguishing healthy and deficient crypts involves assessing their visual characteristics under a microscope, with healthy crypts appearing brown and ACF crypts blue. Other distinguishing criteria are explored in this project. Yet, the rarity of ACF crypts coupled with the manual microscopic examination by specialists - around 15 minutes per slide - hinders efficient diagnosis, with a single patient\u0026rsquo;s results taking around 2.5 hours due to multiple slides.\u003c/p\u003e\n\u003cp\u003eThis project\u0026rsquo;s primary objective is to harness deep learning and machine learning techniques to aid specialists in detecting enterocytes and distinguishing different crypt types. This initiative aims to expedite diagnosis, facilitating swifter patient intervention during the onset of disease symptoms. Building upon the work of ClÃ©mence Lanfranchi [reference], we seek to employ advanced computational methods to enhance the diagnostic process.\u003c/p\u003e\n\u003cp\u003eThe project unfolds in several phases:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMedical Understanding:\u003c/strong\u003e We begin by delving into the medical definition of Lynch Syndrome [reference], improving our comprehension of this condition and the intricacies of its diagnosis.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Preprocessing:\u003c/strong\u003e We explore the data preprocessing steps, detailing the crypt segmentation methodology from ClÃ©mence Lanfranchi\u0026rsquo;s work [reference]. We also outline the ranking technique employed for distinguishing crypt types. The dataset and its preprocessing modifications are introduced, addressing the challenge of limited deficient class samples.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExperimental Protocol:\u003c/strong\u003e We present the setup of our experimental protocol, elaborating on the classification algorithms employed and their outcomes. Deep learning methods play a crucial role in our approach.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFuture Prospects:\u003c/strong\u003e The project concludes with a discussion on potential improvements, both in introducing advanced machine learning techniques and exploring alternative physiological-histological discriminating factors beyond the scope of ClÃ©mence Lanfranchi\u0026rsquo;s work [reference]. Additionally, we contemplate the project\u0026rsquo;s impact on environmental, societal, and ethical domains.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy integrating cutting-edge deep learning techniques into medical diagnosis, this project strives to fasten identification of Lynch Syndrome, ultimately leading to more timely and effective patient care.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/project_reports/Projet_MDL.pdf\"\u003e\u003cem\u003e\u003cstrong\u003eRead full report\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n","description":"","image":"/images/projects/Lynch_projet.png","permalink":"https://hugo-profile.netlify.app/projects/lynch/","title":"Machine Learning for the detection of Deficient MMR Crypts to aid in the diagnosis of Lynch Disease"}]